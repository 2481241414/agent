import pandas as pd
import os
import json
import ast
import numpy as np
import math
from tqdm import tqdm
from collections import defaultdict
import time
import itertools
import jieba

# ==============================================================================
# åŒºåŸŸ 1: æ£€æŸ¥å¹¶å¯¼å…¥æ‰€éœ€åº“
# ==============================================================================
try:
    from sentence_transformers import SentenceTransformer, util
    import torch
    from rank_bm25 import BM25Okapi
    from sklearn.metrics import roc_auc_score
except ImportError as e:
    print(f"é”™è¯¯: ç¼ºå°‘å¿…è¦çš„åº“ -> {e}")
    print("è¯·åœ¨ç»ˆç«¯è¿è¡Œ: pip install torch sentence-transformers transformers rank_bm25 scikit-learn pandas tqdm")
    exit()

# ==============================================================================
# åŒºåŸŸ 2: å¬å›å™¨ç±»å®šä¹‰
# ==============================================================================

class BM25Retriever:
    """BM25å¬å›å™¨ï¼Œä¸“æ³¨äºå…³é”®è¯å’Œç”¨æˆ·å¤šæ ·åŒ–è¡¨è¾¾çš„åŒ¹é…ã€‚"""
    def __init__(self, data_df: pd.DataFrame, all_tools_definitions: list, k1=1.5, b=0.75):
        self.definitions = all_tools_definitions
        self._add_jieba_words()
        
        print("--- æ­£åœ¨ä¸ºBM25æ„å»ºå…³é”®è¯å¢å¼ºè¯­æ–™åº“... ---")
        corpus = self._build_keyword_rich_corpus(data_df)
        tokenized_corpus = [jieba.lcut(doc, cut_all=False) for doc in tqdm(corpus, desc="BM25è¯­æ–™åº“åˆ†è¯")]
        self.bm25 = BM25Okapi(tokenized_corpus, k1=k1, b=b)
        print("--- BM25å¬å›å™¨æ„å»ºå®Œæˆ ---")

    def _add_jieba_words(self):
        for tool in self.definitions:
            # å°†å‡½æ•°åæœ¬èº«ä¹ŸåŠ å…¥è¯å…¸ï¼Œæé«˜åˆ†è¯å‡†ç¡®æ€§
            jieba.add_word(tool.get('name', '').split('(')[0], freq=100)
        core_words = ["è´­ç‰©è½¦", "é‡‡è´­è½¦", "å¾…æ”¶è´§", "å¾…ä»˜æ¬¾", "æ”¶è—å¤¹", "é™ä»·", "ç­¾åˆ°", "ç§¯åˆ†", "å‘ç¥¨", "å¼€ç¥¨", "æŠ¥é”€å‡­è¯", "ä¼˜æƒ åˆ¸"]
        for word in core_words:
            jieba.add_word(word, freq=100)

    def _build_keyword_rich_corpus(self, data_df: pd.DataFrame) -> list:
        tool_text_aggregator = defaultdict(list)
        for _, row in data_df.iterrows():
            if not isinstance(row.get('ground_truth_tool'), list) or not row['ground_truth_tool']:
                continue
            tool_name = row['ground_truth_tool'][0]['name']
            # èšåˆç”¨æˆ·çœŸå®é—®æ³•(query)å’ŒæŒ‡ä»¤æ¨¡æ¿(æŒ‡ä»¤)
            # if pd.notna(row['query']):
            #     tool_text_aggregator[tool_name].append(row['query'])
            if pd.notna(row['æŒ‡ä»¤']):
                tool_text_aggregator[tool_name].append(row['æŒ‡ä»¤'])
        
        corpus = []
        for tool_def in self.definitions:
            tool_name = tool_def['name']
            description = tool_def.get('description', '')
            func_name_text = tool_name.split('(')[0].replace('_', ' ')
            aggregated_text = ' '.join(set(tool_text_aggregator.get(tool_name, [])))
            # æ„å»ºä¸€ä¸ªåŒ…å«åŠŸèƒ½ã€å‡½æ•°åå’Œç”¨æˆ·å„ç§è¯´æ³•çš„ä¸°å¯Œæ–‡æ¡£
            document = f"åŠŸèƒ½æè¿°: {description} {func_name_text}ã€‚ç”¨æˆ·çœŸå®é—®æ³•å’ŒæŒ‡ä»¤: {aggregated_text}"
            corpus.append(document)
        return corpus

    def retrieve_scores(self, query: str) -> np.ndarray:
        tokenized_query = jieba.lcut(query, cut_all=False)
        return self.bm25.get_scores(tokenized_query)

class SemanticRetriever:
    """è¯­ä¹‰å¬å›å™¨ï¼Œä½¿ç”¨Qwen3æ¨¡å‹è®¡ç®— plan å’Œ æŒ‡ä»¤ ä¹‹é—´çš„æ„å›¾ç›¸ä¼¼åº¦ã€‚"""
    def __init__(self, data_df: pd.DataFrame, all_tools_definitions: list, model_name: str):
        self.definitions = all_tools_definitions
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"--- è¯­ä¹‰å¬å›å™¨æ­£åœ¨ä½¿ç”¨è®¾å¤‡: {self.device} ---")
        print(f"--- æ­£åœ¨åŠ è½½è¯­ä¹‰æ¨¡å‹: {model_name} (é¦–æ¬¡è¿è¡Œéœ€ä¸‹è½½æ¨¡å‹ï¼Œè¯·è€å¿ƒç­‰å¾…) ---")
        
        # åŠ è½½æ¨¡å‹, trust_remote_code=True å¯¹è®¸å¤šæ–°æ¨¡å‹æ˜¯å¿…éœ€çš„
        self.model = SentenceTransformer(model_name, trust_remote_code=True, device=self.device)
        
        print("--- æ­£åœ¨ä¸ºè¯­ä¹‰å¬å›å™¨æ„å»º 'æŒ‡ä»¤' æ¨¡æ¿è¯­æ–™åº“... ---")
        tool_to_template = {}
        # ä»åå¾€å‰éå†æ•°æ®ï¼Œå¯ä»¥ä¼˜å…ˆä½¿ç”¨æ•°æ®é›†ä¸­æ›´é åçš„ã€å¯èƒ½æ›´å¤æ‚çš„æŒ‡ä»¤æ¨¡æ¿
        for _, row in data_df.iloc[::-1].iterrows():
            if not isinstance(row.get('ground_truth_tool'), list) or not row['ground_truth_tool']:
                continue
            tool_name = row['ground_truth_tool'][0]['name']
            if pd.notna(row['æŒ‡ä»¤']):
                tool_to_template[tool_name] = row['æŒ‡ä»¤']
        
        # æŒ‰ç…§ all_tools_definitions çš„å›ºå®šé¡ºåºï¼Œå‡†å¤‡å¾…ç¼–ç çš„æ¨¡æ¿åˆ—è¡¨
        # å¦‚æœæŸä¸ªå·¥å…·åœ¨æ•°æ®ä¸­æ²¡æœ‰å¯¹åº”çš„æŒ‡ä»¤ï¼Œå°±ç”¨å®ƒçš„descriptionä½œä¸ºåå¤‡
        self.ordered_templates = [tool_to_template.get(tool['name'], tool['description']) for tool in self.definitions]
        
        # å¯¹æ‰€æœ‰æŒ‡ä»¤æ¨¡æ¿è¿›è¡Œä¸€æ¬¡æ€§ç¼–ç 
        print("--- æ­£åœ¨å°†æ‰€æœ‰ 'æŒ‡ä»¤' æ¨¡æ¿ç¼–ç ä¸ºå‘é‡... ---")
        self.template_embeddings = self.model.encode(
            self.ordered_templates, convert_to_tensor=True, show_progress_bar=True, device=self.device
        )
        print("--- Qwen3è¯­ä¹‰å¬å›å™¨æ„å»ºå®Œæˆ ---")

    def retrieve_scores(self, query: str) -> np.ndarray:
        query_embedding = self.model.encode(query, convert_to_tensor=True, device=self.device)
        # è®¡ç®—æŸ¥è¯¢(plan)ä¸æ‰€æœ‰æŒ‡ä»¤æ¨¡æ¿çš„ä½™å¼¦ç›¸ä¼¼åº¦
        cos_scores = util.cos_sim(query_embedding, self.template_embeddings)[0]
        return cos_scores.cpu().numpy()

# ==============================================================================
# åŒºåŸŸ 3: è¯„æµ‹å‡½æ•°
# ==============================================================================
def _get_tool_names(tools: list) -> set:
    if not isinstance(tools, list): return set()
    return {tool.get('name') for tool in tools}

def calculate_recall_at_k(retrieved: list, ground_truth: list, k: int) -> float:
    if not ground_truth: return 1.0
    retrieved_names_at_k = _get_tool_names(retrieved[:k])
    ground_truth_names = _get_tool_names(ground_truth)
    if not ground_truth_names: return 1.0
    return len(retrieved_names_at_k.intersection(ground_truth_names)) / len(ground_truth_names)

def calculate_completeness_at_k(retrieved: list, ground_truth: list, k: int) -> float:
    if not ground_truth: return 1.0
    retrieved_names_at_k = _get_tool_names(retrieved[:k])
    ground_truth_names = _get_tool_names(ground_truth)
    return 1.0 if ground_truth_names.issubset(retrieved_names_at_k) else 0.0

def calculate_ndcg_at_k(retrieved: list, ground_truth: list, k: int) -> float:
    ground_truth_names = _get_tool_names(ground_truth)
    if not ground_truth_names: return 1.0
    dcg = sum(1.0 / math.log2(i + 2) for i, tool in enumerate(retrieved[:k]) if tool.get('name') in ground_truth_names)
    idcg = sum(1.0 / math.log2(i + 2) for i in range(min(len(ground_truth_names), k)))
    return dcg / idcg if idcg > 0 else 0.0

def calculate_hit_ratio_at_k(retrieved: list, ground_truth: list, k: int) -> float:
    retrieved_names = _get_tool_names(retrieved[:k])
    return 1.0 if retrieved_names & _get_tool_names(ground_truth) else 0.0

def calculate_average_precision_at_k(retrieved: list, ground_truth: list, k: int) -> float:
    gt_names = _get_tool_names(ground_truth)
    if not gt_names: return 1.0
    hit_count = 0
    sum_prec = 0.0
    for i, tool in enumerate(retrieved[:k]):
        if tool.get('name') in gt_names:
            hit_count += 1
            sum_prec += hit_count / (i + 1)
    return sum_prec / len(gt_names)

def calculate_mrr_at_k(retrieved: list, ground_truth: list, k: int) -> float:
    gt_names = _get_tool_names(ground_truth)
    for i, tool in enumerate(retrieved[:k]):
        if tool.get('name') in gt_names:
            return 1.0 / (i + 1)
    return 0.0

def calculate_auc_for_query(all_scores: np.ndarray, tool_defs: list, ground_truth: list) -> float:
    gt_names = _get_tool_names(ground_truth)
    labels = [1 if t['name'] in gt_names else 0 for t in tool_defs]
    try:
        if len(set(labels)) < 2: return 0.5
        return roc_auc_score(labels, all_scores)
    except ValueError: return 0.5

# ==============================================================================
# åŒºåŸŸ 4: å·¥å…·å®šä¹‰
# ==============================================================================
def get_exact_tool_definitions():
    tools = [
        {"name": "search_goods(app, search_info_slot, page_type, filter_detail_slot, type_slot, area_slot, order_type)", "description": "åœ¨appç¨‹åºä¸­ä¾æ®åç§°æœç´¢å•†å“,å¯ä»¥æŒ‡å®šå…·ä½“åœ¨å“ªä¸€ä¸ªå­é¡µé¢è¿›è¡Œæœç´¢, æœç´¢ç»“æœçš„ç­›é€‰æ¡ä»¶å’Œæ’åºæ–¹å¼"},
        {"name": "search_stores(app, search_info_slot, filter_type, filter_detail_slot, location_slot, qualification_slot, order_type)", "description": "åœ¨appç¨‹åºä¸­ä¾æ®åç§°æœç´¢åº—é“º,å¯ä»¥ä½¿ç”¨ç­›é€‰å™¨é™åˆ¶æœç´¢ç»“æœ,ä¹Ÿå¯ä»¥æŒ‡å®šæœç´¢ç»“æœçš„æ’åºæ–¹å¼"},
        {"name": "open_search_history(app)", "description": "æ‰“å¼€appç¨‹åºçš„æœç´¢å†å²ç•Œé¢"},
        {"name": "delete_search_history(app)", "description": "æ¸…é™¤appä¸­çš„æœç´¢å†å²"},
        {"name": "open_camera_search(app)", "description": "æ‰“å¼€appç¨‹åºçš„å›¾ç‰‡æœç´¢åŠŸèƒ½"},
        {"name": "search_delivery_time(app, search_info_slot, address_slot)", "description": "æœç´¢ä¸€ä»¶å•†å“å¹¶æ ¹æ®ç»™å‡ºçš„åœ°å€æŸ¥è¯¢è¯¥å•†å“é€è¾¾è¯¥åœ°å€çš„é¢„ä¼°è¿é€æ—¶é—´"},
        {"name": "search_cart_content(app, search_info_slot)", "description": "åœ¨appåº”ç”¨ç¨‹åºä¸­æŸ¥çœ‹è´­ç‰©è½¦/é‡‡è´­è½¦(é˜¿é‡Œå·´å·´çš„å«æ³•)æŸ¥æ‰¾å•†å“"},
        {"name": "search_in_favorite_goods(app, search_info_slot)", "description": "åœ¨appç¨‹åºä¸­æ‰“å¼€æ”¶è—çš„ã€å–œçˆ±ã€æƒ³è¦æˆ–å…³æ³¨å•†å“çš„é¡µé¢,å¹¶åœ¨å…¶ä¸­çš„æœç´¢æ ä¸­è¿›è¡Œæœç´¢"},
        {"name": "search_in_favorite_stores(app, search_info_slot)", "description": "åœ¨appç¨‹åºä¸­æ‰“å¼€æ”¶è—çš„å–œçˆ±æˆ–å…³æ³¨åº—é“ºçš„é¡µé¢,å¹¶åœ¨å…¶ä¸­çš„æœç´¢æ æœç´¢å•†å“"},
        {"name": "search_order(app, search_info_slot, order_status)", "description": "åœ¨appåº”ç”¨ç¨‹åºä¸­æœç´¢è®¢å•"},
        {"name": "open_goods_page(app, search_info_slot, page_type)", "description": "é€šè¿‡å•†å“åç§°æ‰¾åˆ°å¹¶æ‰“å¼€å…¶è¯¦æƒ…é¡µé¢,å¯ä»¥æŒ‡å®šå­é¡µé¢,ä¾‹å¦‚è¯„è®ºã€è§„æ ¼ã€å‚æ•°ã€è¯¦æƒ…ç­‰"},
        {"name": "open_stores_page(app, store_name_slot, search_info_slot, category_slot)", "description": "é€šè¿‡åº—é“ºåç§°æ‰¾åˆ°å¹¶æ‰“å¼€åº—é“ºçš„å†…å®¹é¡µé¢,å¯ä»¥åœ¨å…¶ä¸­è¿›è¡Œåº—é“ºå†…æœç´¢æˆ–æ‰“å¼€ç±»åˆ«å­é¡µé¢"},
        {"name": "open_special_page(app, page_type)", "description": "æ‰“å¼€ç‰¹æ®Šé¡µé¢,ä¾‹å¦‚æ´»åŠ¨é¡µé¢"},
        {"name": "open_cart_content(app, filter_type, filter_detail_slot)", "description": "åœ¨appåº”ç”¨ç¨‹åºä¸­æŸ¥çœ‹è´­ç‰©è½¦/é‡‡è´­è½¦(é˜¿é‡Œå·´å·´çš„å«æ³•)æŒ‡å®šç±»å‹çš„å•†å“"},
        {"name": "add_into_cart(app, search_info_slot, specification_slot, num_slot, address_slot)", "description": "æœç´¢å•†å“å¹¶å°†å…¶æ·»åŠ å…¥è´­ç‰©è½¦,å¯ä»¥æŒ‡å®šæ·»åŠ çš„å•†å“è§„æ ¼ã€æ•°é‡å¹¶é€‰æ‹©æ”¶è´§åœ°å€"},
        {"name": "open_favorite_goods(app, filter_type, filter_detail_slot, order_type)", "description": "åœ¨appç¨‹åºä¸­æ‰“å¼€æ”¶è—çš„å–œçˆ±ã€æƒ³è¦æˆ–å…³æ³¨å•†å“çš„é¡µé¢,å¹¶æŒ‰ç…§æ¡ä»¶è¿›è¡Œç­›é€‰"},
        {"name": "open_favorite_stores(app, filter_type)", "description": "åœ¨appç¨‹åºä¸­æ‰“å¼€æ”¶è—çš„å–œçˆ±æˆ–å…³æ³¨åº—é“ºçš„é¡µé¢,å¹¶æŒ‰ç…§æ¡ä»¶è¿›è¡Œç­›é€‰"},
        {"name": "add_into_favorite_goods(app, search_info_slot)", "description": "åœ¨appç¨‹åºä¸­æœç´¢å•†å“,å¹¶å°†å…¶æ·»åŠ åˆ°å•†å“æ”¶è—å¤¹ä¸­"},
        {"name": "add_into_favorite_stores(app, search_info_slot)", "description": "åœ¨appç¨‹åºä¸­æŒ‰ç…§åº—é“ºåæœç´¢åº—é“º,å¹¶å°†å…¶æ·»åŠ åˆ°åº—é“ºæ”¶è—å¤¹ä¸­"},
        {"name": "delete_favorite_goods(app, search_info_slot)", "description": "åœ¨appç¨‹åºçš„å•†å“æ”¶è—å¤¹ä¸­æœç´¢æŒ‡å®šå•†å“å¹¶å°†å…¶åˆ é™¤"},
        {"name": "order_to_purchase_goods(app, search_info_slot, specification_slot, num_slot, address_slot, payment_method_slot)", "description": "é€šè¿‡å•†å“åç§°æ‰¾åˆ°å•†å“å¹¶ä¸‹å•è´­ä¹°,å¯ä»¥æŒ‡å®šæ·»åŠ çš„å•†å“è§„æ ¼ã€æ•°é‡å¹¶é€‰æ‹©æ”¶è´§åœ°å€ä»¥åŠæ”¯ä»˜æ–¹å¼"},
        {"name": "open_orders_bought(app, order_status, filter_detail_slot)", "description": "åœ¨appåº”ç”¨ç¨‹åºä¸­æŸ¥çœ‹ä¹°å…¥çš„æŒ‡å®šçŠ¶æ€çš„è®¢å•åˆ—è¡¨,ä¾‹å¦‚å¾…ä»˜æ¬¾ã€å¾…æ”¶è´§ã€å¾…è¯„ä»·ç­‰ã€‚"},
        {"name": "open_orders_sold(app, order_status, filter_detail_slot)", "description": "åœ¨appåº”ç”¨ç¨‹åºä¸­æŸ¥çœ‹è‡ªå·±å”®å–çš„æŒ‡å®šçŠ¶æ€çš„è®¢å•åˆ—è¡¨,ä¾‹å¦‚å¾…ä»˜æ¬¾ã€å¾…æ”¶è´§ã€å¾…è¯„ä»·ç­‰ã€‚"},
        {"name": "open_orders_release(app, order_status)", "description": "åœ¨appåº”ç”¨ç¨‹åºä¸­æŸ¥çœ‹è‡ªå·±å‘å¸ƒçš„æŒ‡å®šçŠ¶æ€çš„è®¢å•åˆ—è¡¨,ä¾‹å¦‚åœ¨å–ã€è‰ç¨¿ã€å·²ä¸‹æ¶ç­‰ã€‚"},
        {"name": "open_orders_all_review(app)", "description": "åœ¨appåº”ç”¨ç¨‹åºä¸­æŸ¥çœ‹å¾…è¯„ä»·çŠ¶æ€çš„è®¢å•åˆ—è¡¨,åœ¨ä¸æŒ‡å®šè´­ä¹°è¿˜æ˜¯å”®å–çš„è®¢å•æ—¶,åŠå…¨éƒ½è¦çœ‹æ—¶ä½¿ç”¨ã€‚"},
        {"name": "apply_after_sales(app, search_info_slot, after_sales_type, reason_slot)", "description": "åœ¨appåº”ç”¨ç¨‹åºä¸­æœç´¢è®¢å•,å¹¶ç”³è¯·å”®å"},
        {"name": "open_logistics_receive(app, filter_type)", "description": "æ‰“å¼€æ˜¾ç¤ºå·²è´­å•†å“ä¿¡æ¯çš„ç•Œé¢,æŸ¥çœ‹ç›¸å…³ç‰©æµä¿¡æ¯,å¹¶æ ¹æ®ç‰©æµæƒ…å†µè¿›è¡Œç­›é€‰"},
        {"name": "open_logistics_send(app, filter_type)", "description": "æ‰“å¼€æ˜¾ç¤ºå·²å”®å•†å“ä¿¡æ¯çš„ç•Œé¢,æŸ¥çœ‹ç›¸å…³ç‰©æµä¿¡æ¯,å¹¶æ ¹æ®ç‰©æµæƒ…å†µè¿›è¡Œç­›é€‰"},
        {"name": "open_express_delivery(app)", "description": "æ‰“å¼€appå¯„é€å¿«é€’çš„ç•Œé¢"},
        {"name": "manage_order_logistics_status(app, search_info_slot, action_type)", "description": "åœ¨appä¸­ç®¡ç†æŒ‡å®šè®¢å•çš„ç‰©æµçŠ¶æ€,åŒ…æ‹¬å‚¬å‘è´§,å‚¬é…é€,ç¡®è®¤æ”¶è´§"},
        {"name": "open_order_tracking_number(app, search_info_slot)", "description": "åœ¨appä¸­æŸ¥è¯¢æŒ‡å®šè®¢å•çš„ç‰©æµå•å·"},
        {"name": "call_order_courier(app, search_info_slot)", "description": "åœ¨appä¸­æ‹¨æ‰“æŒ‡å®šè®¢å•çš„å¿«é€’ç”µè¯"},
        {"name": "open_customer_service(app, order_slot, store_slot)", "description": "åœ¨appåº”ç”¨ç¨‹åºä¸­è”ç³»å®˜æ–¹å®¢æœ,æˆ–è”ç³»æŒ‡ä»¤è®¢å•çš„åº—é“ºå®¢æœ,æˆ–è”ç³»æŒ‡å®šåº—é“ºçš„å®¢æœ"},
        {"name": "apply_price_protection(app)", "description": "åœ¨appåº”ç”¨ç¨‹åºä¸­è”ç³»å®¢æœè¿›è¡Œä»·ä¿"},
        {"name": "rate_order(app, search_info_slot, rating_slot, review_text_slot, upload_images)", "description": "åœ¨appåº”ç”¨ç¨‹åºè¯„ä»·å•†åŸä¸­çš„æŒ‡å®šè®¢å•"},
        {"name": "open_invoice_page(app, page_type)", "description": "åœ¨appåº”ç”¨ç¨‹åºä¸­æ‰“å¼€ä¸å‘ç¥¨ç›¸å…³çš„é¡µé¢"},
        {"name": "sign_in(app, page_type)", "description": "åœ¨appç¨‹åºä¸­å®Œæˆæ¯æ—¥ç­¾åˆ°,é¢†å–ç§¯åˆ†ã€é‡‘å¸ç­‰å¥–åŠ±çš„æ“ä½œ"},
        {"name": "open_app(app)", "description": "æ‰“å¼€æŒ‡å®šçš„åº”ç”¨ç¨‹åº"},
    ]
    return tools

# ==============================================================================
# åŒºåŸŸ 5: ä¸»ç¨‹åº - æ··åˆæ£€ç´¢
# ==============================================================================
def main():
    # --- 0. é…ç½®åŒºåŸŸ ---
    annotated_data_file_path = '/home/workspace/lgq/shop/data/single_gt_output_with_plan.csv'
    # ã€æ ¸å¿ƒã€‘ä½¿ç”¨æ‚¨æŒ‡å®šçš„ Qwen3-Embedding-0.6B æ¨¡å‹
    SEMANTIC_MODEL_NAME = '/home/workspace/lgq/shop/model/Qwen3-Embedding-0.6B'
    
    ALPHA = 0.5  # èåˆæƒé‡: 0.5ä»£è¡¨BM25å’Œè¯­ä¹‰å„å ä¸€åŠã€‚å¯ä»¥åœ¨0.3-0.7ä¹‹é—´è°ƒæ•´
    K_VALUES = [1, 2, 3, 5, 10]
    NUM_ERROR_EXAMPLES_TO_PRINT = 10

    # --- 1. æ•°æ®åŠ è½½ ---
    print("--- æ­¥éª¤ 1: åŠ è½½å®Œæ•´æ•°æ®é›† ---")
    try:
        required_columns = ['query', 'æŒ‡ä»¤', 'planï¼ˆåœ¨xxä¸­åšä»€ä¹ˆï¼‰', 'ground_truth_tool']
        data_df = pd.read_csv(annotated_data_file_path, usecols=required_columns).dropna().reset_index(drop=True)
        def parse_tools(s): return ast.literal_eval(s) if isinstance(s, str) else []
        data_df['ground_truth_tool'] = data_df['ground_truth_tool'].apply(parse_tools)
        print(f"æ•°æ®åŠ è½½å®Œæˆ: å…± {len(data_df)} æ¡ã€‚\n")
    except FileNotFoundError:
        print(f"é”™è¯¯: æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œè¯·æ£€æŸ¥è·¯å¾„ '{annotated_data_file_path}'")
        return
    except Exception as e:
        print(f"é”™è¯¯: è¯»å–æˆ–è§£ææ–‡ä»¶ '{annotated_data_file_path}' å¤±è´¥. {e}")
        return

    # --- 2. åˆå§‹åŒ–åŒè·¯å¬å›å™¨ ---
    all_tools_definitions = get_exact_tool_definitions()
    bm25_retriever = BM25Retriever(data_df, all_tools_definitions)
    semantic_retriever = SemanticRetriever(data_df, all_tools_definitions, model_name=SEMANTIC_MODEL_NAME)
    
    # --- 3. è¯„æµ‹æ··åˆå¬å›æ¨¡å‹ ---
    print(f"\n--- æ­¥éª¤ 3: å¼€å§‹è¯„æµ‹æ··åˆå¬å›æ¨¡å‹ (BM25 + Qwen3, Alpha={ALPHA}) ---")
    results = defaultdict(lambda: defaultdict(list))
    error_cases = []

    for _, row in tqdm(data_df.iterrows(), total=len(data_df), desc="æ··åˆå¬å›è¯„æµ‹ä¸­"):
        query = row['planï¼ˆåœ¨xxä¸­åšä»€ä¹ˆï¼‰']
        ground_truth = row['ground_truth_tool']
        
        start_time = time.time()
        
        # 1. è·å–ä¸¤è·¯å¬å›çš„åˆ†æ•°
        bm25_scores = bm25_retriever.retrieve_scores(query)
        semantic_scores = semantic_retriever.retrieve_scores(query)
        
        # 2. åˆ†æ•°å½’ä¸€åŒ– (Min-Max Normalization)ï¼Œè¿™æ˜¯èåˆçš„å…³é”®æ­¥éª¤
        def normalize(scores):
            min_s, max_s = scores.min(), scores.max()
            if (max_s - min_s) == 0: return np.zeros_like(scores)
            return (scores - min_s) / (max_s - min_s)
            
        norm_bm25 = normalize(bm25_scores)
        norm_semantic = normalize(semantic_scores)
        
        # 3. åŠ æƒèåˆå¾—åˆ°æœ€ç»ˆåˆ†æ•°
        final_scores = ALPHA * norm_bm25 + (1 - ALPHA) * norm_semantic
        duration = time.time() - start_time
        
        # 4. æ ¹æ®æœ€ç»ˆåˆ†æ•°æ’åºå¹¶è·å–ç»“æœ
        sorted_indices = np.argsort(final_scores)[::-1]
        retrieved = [all_tools_definitions[i] for i in sorted_indices]
        
        # 5. è®¡ç®—å„é¡¹è¯„æµ‹æŒ‡æ ‡
        results['processing_time']['total'].append(duration)
        results['AUC']['all'].append(calculate_auc_for_query(final_scores, all_tools_definitions, ground_truth))
        for k in K_VALUES:
            results['Recall@K'][k].append(calculate_recall_at_k(retrieved, ground_truth, k))
            results['HR@K'][k].append(calculate_hit_ratio_at_k(retrieved, ground_truth, k))
            results['MAP@K'][k].append(calculate_average_precision_at_k(retrieved, ground_truth, k))
            results['MRR@K'][k].append(calculate_mrr_at_k(retrieved, ground_truth, k))
            results['NDCG@K'][k].append(calculate_ndcg_at_k(retrieved, ground_truth, k))
            results['COMP@K'][k].append(calculate_completeness_at_k(retrieved, ground_truth, k))
        
        # 6. é”™è¯¯åˆ†æ
        if calculate_recall_at_k(retrieved, ground_truth, 1) < 1.0:
            gt_name = _get_tool_names(ground_truth).pop() if ground_truth else "N/A"
            pred_name_top1 = retrieved[0].get('name') if retrieved else "N/A"
            error_cases.append({
                "Query": query,
                "Ground Truth": [gt_name],
                "Prediction@1": [pred_name_top1],
                "Prediction@5": [r.get('name') for r in retrieved[:5]]
            })
        
    # --- 4. æ±‡æ€»å¹¶æŠ¥å‘Šç»“æœ ---
    print("\n\n--- æ­¥éª¤ 4: è¯„æµ‹ç»“æœæŠ¥å‘Š ---")
    final_scores_report = {}
    for metric, vals in results.items():
        if metric == 'AUC': final_scores_report['AUC'] = np.mean(vals['all'])
        elif metric == 'processing_time': continue
        else: final_scores_report[metric] = {f"@{k}": np.mean(v) for k, v in vals.items()}
    
    report_df = pd.DataFrame({ m: final_scores_report[m] for m in ['Recall@K', 'HR@K', 'MAP@K', 'MRR@K', 'NDCG@K', 'COMP@K']}).T
    report_df.columns = [f"@{k}" for k in K_VALUES]
    
    print("æ··åˆå¬å›æ¨¡å‹ (BM25 + Qwen3) åœ¨å®Œæ•´æ•°æ®é›†ä¸Šçš„è¯„æµ‹ç»“æœ:")
    print("-" * 70)
    print(report_df.to_string(formatters={col: '{:.4f}'.format for col in report_df.columns}))
    print(f"\n**AUC (å…¨é‡æ’åº ROC AUC)**: {final_scores_report['AUC']:.4f}")
    print("-" * 70)
    
    total_time = np.sum(results['processing_time']['total'])
    avg_time_ms = np.mean(results['processing_time']['total']) * 1000
    qps = len(data_df) / total_time if total_time > 0 else 0
    
    print("\næ€§èƒ½è¯„æµ‹:")
    print("-" * 70)
    print(f"æ ·æœ¬æ€»æ•°: {len(data_df)} æ¡")
    print(f"æ€»è€—æ—¶: {total_time:.4f} ç§’, å¹³å‡æ¯æ¡è€—æ—¶: {avg_time_ms:.4f} æ¯«ç§’, QPS: {qps:.2f}")
    print("-" * 70)
    
    # --- 5. æ‰“å°é”™è¯¯åˆ†ææŠ¥å‘Š ---
    print(f"\n\n--- æ­¥éª¤ 5: Top-1 é”™è¯¯æ¡ˆä¾‹åˆ†æ (å…± {len(error_cases)} ä¸ªé”™è¯¯) ---")
    if not error_cases:
        print("ğŸ‰ æ­å–œï¼åœ¨æ•°æ®é›†ä¸Šæ²¡æœ‰å‘ç° Top-1 é”™è¯¯æ¡ˆä¾‹ï¼")
    else:
        for i, case in enumerate(error_cases[:NUM_ERROR_EXAMPLES_TO_PRINT]):
            print(f"\n--- é”™è¯¯æ¡ˆä¾‹ {i+1}/{len(error_cases)} ---")
            print(f"  [æŸ¥è¯¢ Query]: {case['Query']}")
            print(f"  [çœŸå®å·¥å…· Ground Truth]: {case['Ground Truth']}")
            print(f"  [é¢„æµ‹å·¥å…· Prediction@1]: {case['Prediction@1']}")
            print(f"  [é¢„æµ‹å·¥å…· Prediction@5]: {case['Prediction@5']}")
        if len(error_cases) > NUM_ERROR_EXAMPLES_TO_PRINT:
            print(f"\n... (ä»…æ˜¾ç¤ºå‰ {NUM_ERROR_EXAMPLES_TO_PRINT} ä¸ªé”™è¯¯æ¡ˆä¾‹) ...")
    print("-" * 70)


if __name__ == "__main__":
    main()